{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from seaborn import color_palette\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Hyperparameters\n",
    "_BATCH_NORM_DECAY = 0.9\n",
    "_BATCH_NORM_EPSILON = 1e-05\n",
    "_LEAKY_RELU = 0.1\n",
    "_AHCHORS = [(10, 13), (16,30), (33,23),\n",
    "            (30,61), (62,45), (59,119),\n",
    "            (116,90), (156,198), (373,326)]\n",
    "_MODEL_SIZE = (416,416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "#batch normalization and leaky ReLU\n",
    "\n",
    "def batch_norm(inputs, training, data_format):\n",
    "    return tf.layers.batch_normalization(\n",
    "        inputs = inputs, axis = 1 if data_format == 'channels_first' else 3,\n",
    "        momentum = _BATCH_NORM_DECAY, epsilon = _BATCH_NORM_EPSILON,\n",
    "        scale = True, training = training\n",
    "    )\n",
    "\n",
    "def fixed_padding(inputs, kernel_size, data_format):\n",
    "    pad_total = kernel_size - 1\n",
    "    pad_beg = pad_total // 2\n",
    "    pad_end = pad_total - pad_beg \n",
    "    if data_format == 'channels_first':\n",
    "        padded_inputs = tf.pad(inputs, [[0,0], [0,0],\n",
    "                                        [pad_beg, pad_end],\n",
    "                                        [pad_beg, pad_end]])\n",
    "    else:\n",
    "        padded_inputs = tf.pad(inputs, [[0,0],\n",
    "                                        [pad_beg, pad_end],\n",
    "                                        [pad_beg, pad_end],\n",
    "                                        [0,0]])\n",
    "    return padded_inputs\n",
    "\n",
    "def conv2d_fixed_padding(inputs, filters, kernel_size, data_format, strides = 1):\n",
    "    if strides > 1:\n",
    "        inputs = fixed_padding(inputs, kernel_size, data_format)\n",
    "    return tf.layers.conv2d(\n",
    "        inputs = inputs, filters = filters, kernel_size = kernel_size,\n",
    "        strides = strides, padding = ('SAME' if strides == 1 else 'VAILD'),\n",
    "        use_bias = False, data_format = data_format\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction: Darknet-53\n",
    "\n",
    "def darknet53_residual_block(inputs, filters, training, data_format, strides = 1):\n",
    "    shortcut = inputs \n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters = filters, kernel_size = 1,\n",
    "        strides = strides, data_format=  data_format\n",
    "    )\n",
    "    inputs = batch_norm( inputs, training = training, data_format= data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha = _LEAKY_RELU)\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters = 2 * filters, kernel_size=3, strides= strides,\n",
    "        data_format= data_format\n",
    "    )\n",
    "    inputs = batch_norm(inputs, training= training, data_format= data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "\n",
    "    inputs += shortcut\n",
    "    return inputs \n",
    "\n",
    "def darknet53(inputs, training, data_format):\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters= 32, kernel_size= 3,\n",
    "        data_format = data_format\n",
    "    )\n",
    "    inputs = batch_norm (inputs, training = training, data_format= data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters = 64, kernel_size= 3, strides= 2, data_format= data_format\n",
    "    )\n",
    "    inputs = batch_norm(inputs, training = training, data_format= data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "    inputs = darknet53_residual_block(inputs, filters = 32, training= training,data_format=data_format)\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters= 128, kernel_size= 3, strides= 2, data_format= data_format\n",
    "    )\n",
    "    inputs = batch_norm(inputs, training= training, data_format= data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "\n",
    "    for _ in range(8):\n",
    "        inputs = darknet53_residual_block(\n",
    "            inputs, filters = 128,\n",
    "            training = training,\n",
    "            data_format = data_format\n",
    "        )\n",
    "\n",
    "        route1 = inputs \n",
    "        inputs = conv2d_fixed_padding(\n",
    "            inputs, filters = 512, kernel_size=3,\n",
    "            strides = 2, data_format = data_format\n",
    "        )\n",
    "        inputs = batch_norm(inputs, training = training, data_format = data_format)\n",
    "        inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "\n",
    "\n",
    "    for _ in range(8):\n",
    "        inputs = darknet53_residual_block(\n",
    "            inputs, filters = 256, training= training, data_format=data_format\n",
    "        )\n",
    "        route2 = inputs \n",
    "        inputs = conv2d_fixed_padding(\n",
    "            inputs, filters = 1024, kernel_size= 3,\n",
    "            strides = 2, data_format = data_format\n",
    "        )\n",
    "        inputs = batch_norm(inputs, training= training, data_format=data_format)\n",
    "        inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "\n",
    "    for _ in range(4):\n",
    "        inputs = darknet53_residual_block(\n",
    "            inputs, filters=512,\n",
    "            training = training,\n",
    "            data_format = data_format\n",
    "        )\n",
    "\n",
    "    return route1, route2, inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolution Layers\n",
    "def yolo_convolution_block(inputs, filters, training, data_format):\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters = filters, kernel_size=1,data_format=data_format\n",
    "    )\n",
    "\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters = 2*filters, kernel_size=3,data_format=data_format\n",
    "    )\n",
    "\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters = filters, kernel_size=1,data_format=data_format\n",
    "    )\n",
    "\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters = 2*filters, kernel_size=3,data_format=data_format\n",
    "    )\n",
    "\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "\n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters = filters, kernel_size=1,data_format=data_format\n",
    "    )\n",
    "\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "\n",
    "    route = inputs \n",
    "    \n",
    "    inputs = conv2d_fixed_padding(\n",
    "        inputs, filters = 2*filters, kernel_size=3,data_format=data_format\n",
    "    )\n",
    "\n",
    "    inputs = batch_norm(inputs, training=training, data_format=data_format)\n",
    "    inputs = tf.nn.leaky_relu(inputs, alpha= _LEAKY_RELU)\n",
    "\n",
    "    return route, inputs \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection Layers\n",
    "\n",
    "def yolo_layer(inputs, n_classes, anchors, img_size, data_format):\n",
    "    n_anchors = len(anchors)\n",
    "\n",
    "    inputs = tf.layers.conv2d(inputs, filters = n_anchors * (5 + n_classes),\n",
    "                              kernel_size = 1, strides = 1, use_bias = True,\n",
    "                              data_format = data_format)\n",
    "    \n",
    "    shape = inputs.get_shape().as_list()\n",
    "\n",
    "    grid_shape = shape[2:4] if data_format == 'channels_first' else shape[1:3]\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0,2,3,1])\n",
    "    inputs = tf.reshape(\n",
    "        inputs, [-1, n_anchors * grid_shape[0] * grid_shape[1],\n",
    "                5 + n_classes]\n",
    "    )\n",
    "\n",
    "    strides = (img_size[0] // grid_shape[0], img_size[1]// grid_shape[1])\n",
    "\n",
    "    box_centers, box_shapes, confidence, classes = tf.split(inputs, [2,2,1,n_classes] , axis = -1)\n",
    "\n",
    "    x = tf.range(grid_shape[0], dtype = tf.float32)\n",
    "    y = tf.range(grid_shape[1], dtype = tf.float32)\n",
    "    x_offset, y_offset = tf.meshgrid(x,y)\n",
    "    x_offset = tf.reshape(x_offset , (-1,1))\n",
    "    y_offset = tf.reshape(y_offset, (-1,1))\n",
    "    x_y_offset = tf.concat([x_offset,y_offset], axis = -1)\n",
    "    x_y_offset = tf.tile(x_y_offset, [1,n_anchors])\n",
    "    x_y_offset = tf.reshape(x_y_offset , [1,-1,2])\n",
    "    box_centers = tf.nn.sigmoid(box_centers)\n",
    "    box_centers = (box_centers + x_y_offset) * strides \n",
    "\n",
    "    anchors = tf.tile(anchors, [grid_shape[0] * grid_shape[1] , 1])\n",
    "    box_shapes = tf.exp(box_shapes) * tf.to_float(anchors)\n",
    "\n",
    "    confidence = tf.nn.sigmoid(confidence)\n",
    "\n",
    "    classes = tf.nn.sigmoid(classes)\n",
    "\n",
    "    inputs = tf.concat([box_centers, box_shapes,\n",
    "                        confidence , classes] , axis = -1)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample layer\n",
    "\n",
    "def upsample(inputs, out_shape, data_format):\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0,2,3,1])\n",
    "        new_height = out_shape[3]\n",
    "        new_width = out_shape[2]\n",
    "    else:\n",
    "        new_height = out_shape[2]\n",
    "        new_width = out_shape[1]\n",
    "\n",
    "    inputs = tf.image.resize_nearest_neightbor(inputs, (new_height, new_width))\n",
    "\n",
    "    if data_format == 'channels_first':\n",
    "        inputs = tf.transpose(inputs, [0,3,1,2])\n",
    "    \n",
    "    return inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-max suppression\n",
    "\n",
    "def build_boxes(inputs):\n",
    "    center_x, center_y, width, height, confidence, classes = tf.split(inputs, [1,1,1,1,1,-1], axis = -1)\n",
    "\n",
    "    top_left_x = center_x - width/2\n",
    "    top_left_y = center_y - height/2\n",
    "    bottom_right_x = center_x + width/2\n",
    "    bottom_right_y = center_y + height/2\n",
    "\n",
    "    boxes = tf.concat(\n",
    "        [top_left_x, top_left_y,\n",
    "         bottom_right_x, bottom_right_y,\n",
    "         confidence, classes], axis= -1\n",
    "    )\n",
    "\n",
    "    return boxes \n",
    "\n",
    "def non_max_suppression(inputs, n_classes, max_output_size, iou_threshold,confidence_threshold):\n",
    "\n",
    "    batch = tf.unstack(inputs)\n",
    "    boxes_dicts = []\n",
    "    for boxes in batch:\n",
    "        boxes = tf.boolean_mask(boxes, boxes[:,4] > confidence_threshold)\n",
    "        classes = tf.argmax(boxes[:,5:], axis = -1 )\n",
    "        classes = tf.expand_dims(tf.to_float(classes), axis= -1)\n",
    "        boxes = tf.concat([boxes[: , :5], classes], axis= -1)\n",
    "        boxes_dicts = dict()\n",
    "\n",
    "        for cls in range(n_classes):\n",
    "            mask = tf.equal(boxes[:, 5],cls)\n",
    "            mask_shape = mask.get_shape()\n",
    "            if mask_shape.ndims != 0 :\n",
    "                class_boxes = tf.boolean_mask(boxes,mask)\n",
    "                boxes_coords, boxes_conf_scores, _ = tf.split( class_boxes,\n",
    "                                                               [4,1,-1],\n",
    "                                                                axis= -1)\n",
    "                boxes_conf_scores = tf.reshape(boxes_conf_scores, [-1])\n",
    "                indices = tf.image.non_max_suppression(boxes_coords,\n",
    "                                                       boxes_conf_scores,\n",
    "                                                       max_output_size,\n",
    "                                                       iou_threshold)\n",
    "                class_boxes = tf.gather(class_boxes, indices)\n",
    "                boxes_dict[cls] = class_boxes[:, :5]\n",
    "        boxes_dicts.append(boxes_dict)\n",
    "    return boxes_dicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model class\n",
    "class Yolo_v3:\n",
    "    def __init__(self, n_classes, model_size, max_output_size, iou_threshold,\n",
    "                 confidence_threshold, data_format = None):\n",
    "        \n",
    "        if not data_format:\n",
    "            if tf.test.is_built_with_cuba():\n",
    "                data_format = 'channels_first'\n",
    "            else:\n",
    "                data_format = 'channels_last'\n",
    "        self.n_classes = n_classes\n",
    "        self.model_size = model_size\n",
    "        self.max_output_size = max_output_size\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.data_format = data_format\n",
    "    \n",
    "    def __call__(self, inputs, training):\n",
    "        with tf.variable_scope('yolo_v3_model'):\n",
    "            if self.data_format == 'channels_first':\n",
    "                inputs = tf.transpose(input, [0,3,1,2])\n",
    "            inputs = inputs / 255\n",
    "            route1, route2, inputs = darknet53(inpus, training = training, \n",
    "                                               data_format=self.data_format)\n",
    "            \n",
    "            route, inputs = yolo_convolution_block(inputs, filters = 512, \n",
    "                                                   training = training, data_format=self.data_format)\n",
    "            \n",
    "            detect1 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[6:9],\n",
    "                                 img_size = self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "            \n",
    "            inputs = conv2d_fixed_padding(route, filters=256,kernel_size=1,\n",
    "                                          data_format=self.data_format)\n",
    "            \n",
    "            inputs = batch_norm(inputs, training=training,\n",
    "                                data_format=self.data_format)\n",
    "            \n",
    "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "            upsample_size = route2.get_shape().as_list()\n",
    "\n",
    "            inputs = upsample(inputs, out_shape=upsample_size,\n",
    "                              data_format=self.data_format)\n",
    "            axis = 1 if self.data_format == 'channels_first' else 3 \n",
    "\n",
    "            inputs = tf.concat([inputs,route2], axis = axis)\n",
    "            route, inputs = yolo_convolution_block(inputs,filter=256,\n",
    "                                                   training = training, data_format=self.data_format)\n",
    "            detect2 = yolo_layer(inputs, n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[3:6],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "            inputs = conv2d_fixed_padding(route, filters=128,kernel_size=1,\n",
    "                                          data_format=self.data_format)\n",
    "            \n",
    "            inputs = batch_norm(inputs, training=training,\n",
    "                                data_format=self.data_format)\n",
    "            \n",
    "            inputs = tf.nn.leaky_relu(inputs, alpha=_LEAKY_RELU)\n",
    "\n",
    "            upsample_size =route1.get_shjape().as_list()\n",
    "            inputs = upsample(inputs, out_shape=upsample_size,\n",
    "                              data_format=self.data_format)\n",
    "            \n",
    "            inputs = tf.concat([inputs,route1], axis =axis)\n",
    "\n",
    "            route, inputs = yolo_convolution_block(inputs,filters = 128,\n",
    "                                                   training = training, data_format=self.data_format)\n",
    "            detect3 = yolo_layer(inputs,n_classes=self.n_classes,\n",
    "                                 anchors=_ANCHORS[0:3],\n",
    "                                 img_size=self.model_size,\n",
    "                                 data_format=self.data_format)\n",
    "            \n",
    "            inputs = tf.concat([detect1,detect2,detect3], axis = 1)\n",
    "            inputs = build_boxes(inputs)\n",
    "\n",
    "            boxes_dicts = non_max_suppression(inputs, n_classes=self.n_classes,\n",
    "                                              max_output_size=self.max_output_size,\n",
    "                                              iou_threshold=self.iou_threshold,\n",
    "                                              confidence_threshold=self.confidence_threshold)\n",
    "            return boxes_dicts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "\n",
    "def load_images(img_names,model_size):\n",
    "    imgs = []\n",
    "\n",
    "    for img_name in img_names:\n",
    "        img = Image.open(img_name)\n",
    "        img = img.resize(size = model_size)\n",
    "        img = np.array(img, dtype = np.float32)\n",
    "        img = np.expand_dims(img, axis = 0)\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.concatenate(imgs)\n",
    "\n",
    "    return imgs \n",
    "\n",
    "def load_class_name(file_name):\n",
    "    with open(file_name , 'r') as f: # Returns a list of class names from tile name\n",
    "        class_names = f.read().splitlines()\n",
    "    return class_names\n",
    "\n",
    "def draw_boxes(img_names, boxes_dicts, class_names, model_size):\n",
    "    colors = ((np.array(color_palette('hls', 80))* 255)).astype(np.uint8)\n",
    "\n",
    "    for num, img_name, boxes_dicts in zip(range(len(img_names)),\n",
    "                                          img_names,\n",
    "                                          boxes_dicts):\n",
    "        img = Image.open(img_name)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        font = ImageFont.truetype(font = '../input/futur.ttf',\n",
    "                                  size = (img.size[0] + img.size[1])//100)\n",
    "        \n",
    "        resize_factor = (img.size[0]/model_size[0], img.size[1]/model_size[1])\n",
    "\n",
    "\n",
    "        for cls in range(len(class_names)):\n",
    "            boxes = boxes_dicts[cls]\n",
    "            if np.size(boxes) != 0:\n",
    "                color = colors[cls]\n",
    "                for box in boxes:\n",
    "                    xy, confidence = box[:4],box[4]\n",
    "                    xy = [xy[i]*resize_factor[i%2] for i in range(4)]\n",
    "                    x0,y0 = xy[0],xy[1]\n",
    "                    thickness = (img.size[0] + img.size[1])//200\n",
    "                    for t in np.linspace(0,1,thickness) :\n",
    "                        xy[0], xy[1] = xy[0]+t, xy[1]+t \n",
    "                        xy[2], xy[3] = xy[2]-t, xy[3]-t\n",
    "                        draw.rectangle(xy, outline = tuple(color))\n",
    "\n",
    "                    text = '{}{:1f}%'.format(class_names[cls],\n",
    "                                             confidence * 100)\n",
    "                    \n",
    "                    text_size = draw.textsize(text, font = font)\n",
    "                    draw.rectangle([x0,y0-text_size[1], x0 + text_size[0], y0],\n",
    "                                   fill = tuple(color))\n",
    "                    draw.text((x0, y0-text_sizep[1]) ,\n",
    "                              text, fill = 'black',\n",
    "                              font = font)\n",
    "        display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting weights to Tensorflow format\n",
    "\n",
    "def load_weights(variables, file_name): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        variables: A list of tf.Variable to be assigned\n",
    "        file_name: A name of a file containing weights.\n",
    "    Returns:\n",
    "        A list of assign operations.\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as f:\n",
    "        #skip first 5 values containing irrelevant info\n",
    "        np.fromfile(f, dtype = np.int32, count = 5)\n",
    "        weights = np.fromfile(f, dtype = np.float32)\n",
    "\n",
    "        assign_ops = []\n",
    "        ptr = 0\n",
    "\n",
    "        # Load weights for darknet part, Each convolution layer has batch normalization\n",
    "\n",
    "        for i in range(52):\n",
    "            conv_var = variables[5*i]\n",
    "            gamma, beta, mean, variance = variables[5*i + 1:5*i + 5]\n",
    "            batch_norm_vars = [beta, gamma, mean, variance]\n",
    "            \n",
    "            for var in batch_norm_vars:\n",
    "                shape = var.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape[shape]\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(var,var_weights))\n",
    "            shape = conv_var.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape((shape[3], shape[2], shape[0], shape[1]))\n",
    "            var_weights = np.transpose(var_weights, (2,3,1,0))\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(conv_var , var_weights))\n",
    "\n",
    "        for j in range(3) :\n",
    "            for i in ranges[j]:\n",
    "                current = 52*5 + 5*i + j*2\n",
    "                conv_var = variables[current]\n",
    "                gamma, beta, mean, variance = variables[current + 1:current + 5]\n",
    "                batch_norm_vars = [beta, gamma, mean, variance]\n",
    "\n",
    "                for var in batch_norm_vars:\n",
    "                    shape = var.shape.as_list()\n",
    "                    num_params = np.prod(shape)\n",
    "                    var_weights = weights[ptr:ptr + num_params].reshape(shape)\n",
    "                    ptr += num_params\n",
    "                    assign_ops.append(tf.assign(var, var_weights))\n",
    "\n",
    "                shape = conv_var.shape.as_list()\n",
    "                num_params = np.prod(shape)\n",
    "                var_weights = weights[ptr:ptr + num_params].reshape((shape[3], shape[2], shape[0], shape[1]))\n",
    "                var_weights = np.transpose(var_weights, (2,3,1,0))\n",
    "                ptr += num_params\n",
    "                assign_ops.append(tf.assign(conv_var , var_weights))\n",
    "            \n",
    "            bias = variables[ 52*5 + unnormalizd[j]*5 + j*2 + 1 ]\n",
    "            shape = bias.shape.as_list()\n",
    "            num_params = np.prod(shape)\n",
    "            var_weights = weights[ptr:ptr + num_params].reshape((shape[3], shape[2], shape[0], shape[1]))\n",
    "            var_weights = np.transpose(var_weights, (2,3,1,0))\n",
    "            ptr += num_params\n",
    "            assign_ops.append(tf.assign(conv_var , var_weights))\n",
    "    return assign_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
